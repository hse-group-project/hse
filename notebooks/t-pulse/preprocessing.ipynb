{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46a9f59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import connection\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import ast\n",
    "from razdel import sentenize\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ff16b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        word   num  percent_from_all_words\n",
      "1      акция  7040                     2.0\n",
      "31      рост  3385                     1.0\n",
      "221      год  3107                     0.9\n",
      "19      цена  2761                     0.8\n",
      "220    рынок  2365                     0.7\n",
      "..       ...   ...                     ...\n",
      "102     вниз   532                     0.2\n",
      "405      пао   523                     0.2\n",
      "356  ожидать   515                     0.1\n",
      "215  которые   515                     0.1\n",
      "374    отчёт   513                     0.1\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"ru_core_news_sm\")\n",
    "russian_stopwords = set(list(set(stopwords.words(\"russian\"))) + ['это', 'р', 'г', 'руб', 'шт'])\n",
    "\n",
    "def top_100_words(num_posts_by_share, min_letters, max_letters, year):\n",
    "\n",
    "    data = pd.read_sql_query(f\"\"\"\n",
    "        WITH filtered AS (\n",
    "        SELECT\n",
    "            ticker,\n",
    "            inserted,\n",
    "            text,\n",
    "            ROW_NUMBER() OVER (PARTITION BY ticker ORDER BY RANDOM()) AS rn\n",
    "        FROM t_pulse_data\n",
    "        WHERE LENGTH(text) BETWEEN {min_letters} AND {max_letters}\n",
    "            AND DATE_PART('year', inserted) = {year}\n",
    "        )\n",
    "        SELECT ticker, inserted, text\n",
    "        FROM filtered\n",
    "        WHERE rn <= {num_posts_by_share}\n",
    "                             \"\"\", connection())\n",
    "\n",
    "    # -------------------------------\n",
    "    # 1. Очистка текста (оставляем только буквы, убираем цифры и смайлики)\n",
    "    # -------------------------------\n",
    "    def clean_text(text):\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        text = str(text).lower()\n",
    "        text = re.sub(r\"http\\S+|www\\S+\", \" \", text)  # убираем ссылки\n",
    "        text = re.sub(r\"[^а-яё\\s]\", \" \", text)       # оставляем только русские буквы\n",
    "        text = re.sub(r\"\\s+\", \" \", text)             # убираем лишние пробелы\n",
    "        return text.strip()\n",
    "\n",
    "    data['clean_text'] = data['text'].apply(clean_text)\n",
    "\n",
    "    # -------------------------------\n",
    "    # 2. Лемматизация + удаление стоп-слов\n",
    "    # -------------------------------\n",
    "    def lemmatize_no_stop(text):\n",
    "        words = text.split()\n",
    "        doc = nlp(\" \".join(words))\n",
    "        return [t.lemma_ for t in doc if t.text not in russian_stopwords]\n",
    "\n",
    "    all_lemmas = []\n",
    "    for text in data['clean_text']:\n",
    "        all_lemmas.extend(lemmatize_no_stop(text))\n",
    "\n",
    "    # -------------------------------\n",
    "    # 3. Подсчет слов\n",
    "    # -------------------------------\n",
    "    words_dict = {}\n",
    "    for word in all_lemmas:\n",
    "        words_dict[word] = words_dict.get(word, 0) + 1\n",
    "\n",
    "    df_words = pd.DataFrame(list(words_dict.items()), columns=['word', 'num'])\n",
    "\n",
    "    total = df_words['num'].sum()\n",
    "    df_words['percent_from_all_words'] = round(df_words['num'] / total * 100, 1)\n",
    "\n",
    "    # Берем топ-100 слов\n",
    "    df_words = df_words.sort_values(by='num', ascending=False).head(100)\n",
    "\n",
    "    return df_words\n",
    "\n",
    "# Пример вызова\n",
    "result = top_100_words(100, 400, 600, 2025)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b287fbe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['акция', 'рост', 'год', 'цена', 'рынок', 'дивиденд', 'компания', 'день', 'уровень', 'новость', 'анализ', 'пока', 'бумага', 'рубль', 'профиль', 'потенциал', 'сегодня', 'пост', 'неделя', 'поддержка', 'дать', 'пульс', 'быть', 'подпишись', 'портфель', 'список', 'млрд', 'цель', 'график', 'текущий', 'зона', 'снижение', 'индекс', 'месяц', 'актив', 'иир', 'новый', 'технический', 'результат', 'пропустить', 'хороший', 'ставка', 'близкий', 'следующий', 'объём', 'время', 'банк', 'итог', 'идея', 'оставаться', 'первый', 'ждать', 'прибыль', 'россия', 'выше', 'тренд', 'движение', 'общий', 'индикатор', 'свой', 'вывод', 'позиция', 'возможный', 'сопротивление', 'падение', 'биржа', 'такой', 'последний', 'отскок', 'млн', 'вверх', 'лонг', 'сигнал', 'также', 'мой', 'мочь', 'момент', 'мсфо', 'т', 'весь', 'стать', 'покупка', 'купить', 'инвестиция', 'около', 'высоко', 'хотеть', 'очень', 'вырасти', 'фактор', 'финансовый', 'инвестор', 'российский', 'интересный', 'ещё', 'вниз', 'пао', 'ожидать', 'которые', 'отчёт']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['word'].values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aa0176",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783f4965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализация Spacy\n",
    "nlp = spacy.load(\"ru_core_news_sm\")\n",
    "russian_stopwords = set(stopwords.words(\"russian\"))\n",
    "\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \" \", text)\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)  # только буквы\n",
    "    text = re.sub(r\"\\d+\", \" \", text)      # убираем цифры\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def parse_reactions(reactions_str):\n",
    "    try:\n",
    "        reactions = ast.literal_eval(reactions_str)\n",
    "    except:\n",
    "        reactions = []\n",
    "    reaction_dict = {f\"reaction_{r['type']}\": r['count'] for r in reactions if 'type' in r and 'count' in r}\n",
    "    for r in ['buy-up','rocket','not-convinced','get-rid','like','dislike']:\n",
    "        reaction_dict.setdefault(f\"reaction_{r}\", 0)\n",
    "    reaction_dict['total_reactions'] = sum(reaction_dict.values())\n",
    "    return pd.Series(reaction_dict)\n",
    "\n",
    "def lemmatize_no_stop(text):\n",
    "    doc = nlp(text)\n",
    "    return [t.lemma_ for t in doc if t.text not in russian_stopwords and t.is_alpha]\n",
    "\n",
    "def top_words_stats(lemmas):\n",
    "    total_words = len(lemmas)\n",
    "    counts = Counter(lemmas)\n",
    "    stats = {}\n",
    "    for word in top_words_list:\n",
    "        stats[f\"tfidf_{word}\"] = counts.get(word, 0) / total_words if total_words > 0 else 0\n",
    "    stats['top_words_pct'] = sum([stats[f\"tfidf_{w}\"] for w in top_words_list])\n",
    "    return pd.Series(stats)\n",
    "\n",
    "def preprocess_and_aggregate(df, top_words_list):\n",
    "    df = df.copy()\n",
    "\n",
    "    df['clean_text'] = df['text'].apply(clean_text)\n",
    "    df['lemmas'] = df['clean_text'].apply(lemmatize_no_stop)\n",
    "    df['num_words'] = df['lemmas'].apply(len)\n",
    "    df['num_chars'] = df['clean_text'].apply(len)\n",
    "    df['num_sentences'] = df['clean_text'].apply(lambda x: len(list(sentenize(x))))\n",
    "    reactions_df = df['reactions_counters'].apply(parse_reactions)\n",
    "    df = pd.concat([df, reactions_df], axis=1)\n",
    "    df['commentscount'] = pd.to_numeric(df['commentscount'], errors='coerce').fillna(0)\n",
    "    df['comment_strength'] = np.log(np.exp(1) + df['commentscount'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    top_words_df = df['lemmas'].apply(top_words_stats)\n",
    "    df = pd.concat([df, top_words_df], axis=1)\n",
    "\n",
    "    df['inserted'] = pd.to_datetime(df['inserted'], errors='coerce').dt.date\n",
    "    df['date'] = df['inserted'].dt.date\n",
    "    df['ticker'] = df['ticker']\n",
    "\n",
    "    agg_cols = ['num_words','num_chars','num_sentences',\n",
    "                'total_reactions','reaction_buy-up','reaction_rocket','reaction_not-convinced',\n",
    "                'reaction_get-rid','reaction_like','reaction_dislike','comment_strength','top_words_pct'] + \\\n",
    "                [f\"tfidf_{w}\" for w in top_words_list]\n",
    "\n",
    "    daily_features = df.groupby(['date','ticker'])[agg_cols].agg(['sum','mean','min','max'])\n",
    "\n",
    "    # Сброс MultiIndex по колонкам\n",
    "    daily_features.columns = ['_'.join(col).strip() if type(col) is tuple else col for col in daily_features.columns.values]\n",
    "    daily_features = daily_features.reset_index()\n",
    "\n",
    "    return daily_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af6c377a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"sber_pulse.csv\")\n",
    "\n",
    "# top_words_list = result['word'].tolist()\n",
    "\n",
    "# # Получение агрегированных признаков\n",
    "# daily_features = preprocess_and_aggregate(df, top_words_list)\n",
    "\n",
    "# print(daily_features.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f83e6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Найти колонки с _min в названии\n",
    "# min_cols = [col for col in daily_features.columns if col.endswith('_min')]\n",
    "\n",
    "# # Оставляем только те, где есть хоть одно значение больше минимального (т.е. не все одинаково минимальные)\n",
    "# cols_to_drop = [col for col in min_cols if daily_features[col].max() == daily_features[col].min()]\n",
    "\n",
    "# # Удаляем эти колонки\n",
    "# daily_features = daily_features.drop(columns=cols_to_drop)\n",
    "\n",
    "# print(daily_features.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
